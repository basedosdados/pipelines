{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "*Recomendo utilizar de Drive para guarda os dados baixados e particionados.*"
      ],
      "metadata": {
        "id": "sKdxybIH9KL1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variaveis de caminho"
      ],
      "metadata": {
        "id": "bEKhIH0QYs4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "year = 2015\n",
        "link_download = f'https://s3.sa-east-1.amazonaws.com/ckan.saude.gov.br/SISVAN/estado_nutricional/sisvan_estado_nutricional_{year}.zip'\n",
        "path_source_csv = f'/content/input/{year}/sisvan_estado_nutricional_{year}.csv'\n",
        "path_partitioned_folder = f'/content/output/ano={year}'"
      ],
      "metadata": {
        "id": "3C-gdEX0Ywwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importaçoes e funçoes"
      ],
      "metadata": {
        "id": "NOtIv8N2cplt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import io\n",
        "import os\n",
        "import concurrent.futures\n",
        "from zipfile import ZipFile\n",
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import glob\n",
        "import shutil\n",
        "import csv\n",
        "import gc\n",
        "\n",
        "\n",
        "def get_year(link: str) -> str:\n",
        "  year = re.search('[0-9]{4}', link)\n",
        "  return year.group() if year else \"Falha\"\n",
        "\n",
        "def download_extract(link: str) -> None:\n",
        "\n",
        "  r = requests.get(link)\n",
        "  year = get_year(link)\n",
        "  z = ZipFile(io.BytesIO(r.content))\n",
        "  path = os.path.join(os.getcwd(), 'input', year)\n",
        "  z.extractall(path)\n",
        "\n",
        "\n",
        "def create_folders(name_folders: str) -> None:\n",
        "    try:\n",
        "        os.makedirs(name_folders)\n",
        "\n",
        "    except FileExistsError:\n",
        "        pass\n",
        "\n",
        "def extrair(path, path_output) -> None:\n",
        "  # loading the temp.zip and creating a zip object\n",
        "  with ZipFile(path, 'r') as zObject:\n",
        "\n",
        "      # Extracting all the members of the zip\n",
        "      # into a specific location.\n",
        "      zObject.extractall(\n",
        "          path=path_output)\n",
        "\n",
        "def csv_manager(path: str, row: dict, mode: str) -> None:\n",
        "\n",
        "  with open(path, mode, newline='') as file:\n",
        "  # Create a CSV writer object\n",
        "    writer = csv.writer(file)\n",
        "\n",
        "    # Write the field names\n",
        "    if mode == 'w':\n",
        "      writer.writerow(row.keys())\n",
        "    writer.writerow(row.values())\n",
        "\n",
        "def partition_dataset(path_source_dataset: str) -> None:\n",
        "\n",
        "  with open(path_source_dataset, 'r', encoding=\"iso-8859-1\") as file:\n",
        "    reader = csv.DictReader(file, delimiter=';')\n",
        "    year = get_year(path_source_dataset)\n",
        "    for row in reader:\n",
        "      data = row['DT_ACOMPANHAMENTO']\n",
        "      mes = data.split('/')[1]\n",
        "      path_output_csv = os.path.join(os.getcwd(),\n",
        "                                      \"output\", f'ano={year}',\n",
        "                                      f'mes={mes}', f'sigla_uf={row[\"SG_UF\"]}')\n",
        "      file_csv = os.path.join(path_output_csv,\n",
        "                              f'sisvan_nutricional_{row[\"SG_UF\"]}_{mes}_{year}.csv')\n",
        "\n",
        "      if os.path.exists(file_csv):\n",
        "        csv_manager(file_csv, row, 'a')\n",
        "      else:\n",
        "        create_folders(path_output_csv)\n",
        "        csv_manager(file_csv, row, 'w')\n",
        "\n",
        "\n",
        "def verification_dataset_sum(sum_verify: int, row_sum_by_model: int) -> None:\n",
        "  if sum_verify == row_sum_by_model:\n",
        "    print(f\"Processamento feito com sucesso!\\nValidação:\\n\" \\\n",
        "    f\"Soma linhas {sum_verify} | Soma verificadora por meses {row_sum_by_model}\")\n",
        "  else:\n",
        "    print(f\"Processamento falho!\\nValidação:\\n\" \\\n",
        "    f\"Soma linhas {sum_verify} | Soma verificadora por meses {row_sum_by_model}\")\n",
        "    raise ValueError(\"Somas verificadoras não batem\")"
      ],
      "metadata": {
        "id": "AqjCFBm0cx7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download e Particionar"
      ],
      "metadata": {
        "id": "WjNSBrcVy04R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download"
      ],
      "metadata": {
        "id": "G99BpLJIVKEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "download_extract(link_download)"
      ],
      "metadata": {
        "id": "wn6kVXuGVK7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Particionar Dataset"
      ],
      "metadata": {
        "id": "VFDsFNjwVLhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "partition_dataset(path_source_csv)"
      ],
      "metadata": {
        "id": "M60UvmPxel5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validar Particionamento"
      ],
      "metadata": {
        "id": "qBe3yFQCe0hs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ufs = ['SP',\n",
        "    'RS',\n",
        "    'CE',\n",
        "    'MG',\n",
        "    'PE',\n",
        "    'MA',\n",
        "    'RJ',\n",
        "    'PI',\n",
        "    'AL',\n",
        "    'PR',\n",
        "    'BA',\n",
        "    'SC',\n",
        "    'RR',\n",
        "    'RN',\n",
        "    'MT',\n",
        "    'MS',\n",
        "    'AM',\n",
        "    'DF',\n",
        "    'PA',\n",
        "    'SE',\n",
        "    'AP',\n",
        "    'TO',\n",
        "    'ES',\n",
        "    'GO',\n",
        "    'PB',\n",
        "    'RO',\n",
        "    'AC']\n",
        "\n",
        "model_meses = {n: 0 for n in range(1, 13)}\n",
        "\n",
        "modelo_analise_particionado = {uf: model_meses.copy() for uf in ufs}"
      ],
      "metadata": {
        "id": "lcPvV3yXfURy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Criar modelo de verificação Source Dataset"
      ],
      "metadata": {
        "id": "cPp8ckmXiFM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelo_analise = {uf: model_meses.copy() for uf in ufs}\n",
        "\n",
        "# Specify the chunk size\n",
        "chunksize = 3000000\n",
        "\n",
        "# Open the CSV file in chunks\n",
        "row_sum_source_dataset = sum_verify_source_dataset = 0\n",
        "\n",
        "for chunk in pd.read_csv(path_source_csv,\n",
        "                         chunksize=chunksize,  encoding=\"iso-8859-1\", sep=\";\"):\n",
        "    # Process each chunk here\n",
        "    chunk['DT_ACOMPANHAMENTO'] = pd.to_datetime(chunk['DT_ACOMPANHAMENTO'],\n",
        "                                             format='%d/%m/%Y')\n",
        "    chunk['ano'] = chunk['DT_ACOMPANHAMENTO'].dt.year\n",
        "    chunk['mes'] = chunk['DT_ACOMPANHAMENTO'].dt.month\n",
        "\n",
        "    for uf, meses in modelo_analise.items():\n",
        "      for mes in meses.keys():\n",
        "        modelo_analise[uf][mes] += len(chunk[(chunk.SG_UF == uf) & (chunk.mes == mes)])\n",
        "    row_sum_source_dataset += len(chunk)\n",
        "\n",
        "for key, meses in modelo_analise.items():\n",
        "  total = sum(list(meses.values()))\n",
        "  sum_verify_source_dataset += total\n",
        "\n",
        "verification_dataset_sum(row_sum_source_dataset, sum_verify_source_dataset)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8td5iRpfmUV",
        "outputId": "1933a0e7-7787-42d7-b598-2e87e8e3db02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processamento feito com sucesso!\n",
            "Validação:\n",
            "Soma linhas 50544072 | Soma verificadora por meses 50544072\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Criar modelo de verificação Partição"
      ],
      "metadata": {
        "id": "YlCdC1IaiTl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelo_analise_particionado = {uf: model_meses.copy() for uf in ufs}\n",
        "\n",
        "row_sum_partition = sum_verify_partition = 0\n",
        "\n",
        "# Specify the path to the directory\n",
        "path = f\"{path_partitioned_folder}/**/*.csv\"\n",
        "\n",
        "# Use glob() to find all CSV files\n",
        "csv_files = glob.glob(path, recursive=True)\n",
        "\n",
        "for csv in csv_files:\n",
        "\n",
        "  fragment = csv.split('_')\n",
        "  mes = int(fragment[4])\n",
        "  uf = fragment[3]\n",
        "  rows_part = len(pd.read_csv(csv))\n",
        "  modelo_analise_particionado[uf][mes] += rows_part\n",
        "  row_sum_partition += rows_part\n",
        "\n",
        "for key, meses in modelo_analise_particionado.items():\n",
        "  total = sum(list(meses.values()))\n",
        "  sum_verify_partition += total\n",
        "\n",
        "verification_dataset_sum(row_sum_partition, sum_verify_partition)"
      ],
      "metadata": {
        "id": "u4-vyvdqij0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparar Modelos de verificação"
      ],
      "metadata": {
        "id": "v5oSe0nTu5to"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "erros = False\n",
        "for key, meses in modelo_analise_particionado.items():\n",
        "  for mes in range(1,13):\n",
        "    total_particionado = modelo_analise_particionado[key][mes]\n",
        "    total_raw = modelo_analise[key][mes]\n",
        "    if total_particionado != total_raw:\n",
        "      print(f\"Diferença em {key} no mês {mes}\\nParticionado: {total_particionado} Puro: {total_raw}\")\n",
        "      erros = True\n",
        "\n",
        "if erros:\n",
        "  print(f\"Erro!\\nNão pronto para upload. Houve diferenças em algum mês\")\n",
        "  raise ValueError(\"Verifique as diferenças entre os modelos\")\n",
        "else:\n",
        "  print(\"Pasta Output pronto para upload!\")"
      ],
      "metadata": {
        "id": "RLIkLC_Au_93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a130412b-f29b-4114-8821-dd45abd237ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pasta Output pronto para upload!\n"
          ]
        }
      ]
    }
  ]
}
